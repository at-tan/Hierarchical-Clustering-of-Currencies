{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = 'C:\\\\fx_2017_19.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File C:\\fx_2017_19.csv does not exist: 'C:\\\\fx_2017_19.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-eaa612125082>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File C:\\fx_2017_19.csv does not exist: 'C:\\\\fx_2017_19.csv'"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv(file_location)\n",
    "df = pd.DataFrame(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df.Date).dt.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "df.index = df.index.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['usd', 'msci', 'oil', 'cny', 'twd', 'krw', 'sgd', 'myr', 'idr', 'thb', 'php', 'inr', 'rub', \n",
    "                'pln', 'huf', 'ron', 'try', 'brl', 'mxn', 'clp', 'cop', 'zar', 'ils', 'eur', 'gbp',\n",
    "                'jpy', 'aud', 'nzd', 'cad', 'chf', 'nok', 'sek', 'gold', 'silver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the exchange rates are rendered in USD/XXX terms, so they are all expressed as 1 US dollar equivalents. \n",
    "#### The broad USD index used is the Fed nominal broad trade-weighted US dollar exchange rate, and the crude oil price is the Brent crude price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the equivalent 1 US$ values for gold and silver\n",
    "df['au$'] = 1/df.gold\n",
    "df['ag$'] = 1/df.silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['gold', 'silver'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with financial market returns correlations, it is preferable to use the log returns, but the CAPM beta is calculated on the simple percentage returns. \n",
    "\n",
    "#### Furthermore, it is preferable to standardise the data for the clustering algorithms.\n",
    "\n",
    "#### Thus, the order of data transformation will be to obtain the log returns and calculate the correlation coefficients, then get the simple returns and calculate the betas, and finally standardise the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cross-asset correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations are a linear measure, so we use the log returns to reduce the likely distributional skew to FX returns, though FX returns do tend to be less skewed than equities. But we check later below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logret = np.log(df/df.shift())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the frist row of null values\n",
    "df_logret = df_logret.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df_logret.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd_correlations = correlations['usd'].iloc[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of USD correlations\n",
    "usd_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_correlations = correlations['msci'].iloc[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of World equity correlations\n",
    "equity_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CAPM beta is calculated on simple percentage returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple percentage returns to calculate the respective betas\n",
    "df_returns = df.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns = df_returns.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(18, 6))\n",
    "sns.distplot(df_logret['ag$'], bins=20, kde=True, ax=ax1).set_title('USD/Silver log returns histogram')\n",
    "sns.distplot(df_returns['ag$'], bins=20, kde=True, ax=ax2).set_title('USD/Silver simple returns histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms above confirm not much difference between the simple returns distribution compared to the log returns distribution, though this was in a low vol period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate beta\n",
    "def beta(df):\n",
    "    # sets first column as the market data\n",
    "    X = df.values[:, [0]]\n",
    "    # prepend a column of ones for the intercept\n",
    "    X = np.concatenate([np.ones_like(X), X], axis=1)\n",
    "    # matrix algebra for regression coefficient\n",
    "    b = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(df.values[:, 1:])\n",
    "    return pd.Series(b[1], df.columns[1:], name='beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the betas to US dollar\n",
    "usd_betas = beta(df_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usd_betas['msci']\n",
    "del usd_betas['oil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd_betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding implied volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The values are 25-delta 3-month implied volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.read_csv('C:\\\\vol_2017_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v['Date'] = pd.to_datetime(df_v.Date).dt.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v.set_index('Date', inplace=True)\n",
    "df_v.index = df_v.index.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v.columns = ['cny', 'twd', 'krw', 'sgd', 'myr', 'idr', 'thb', 'php', 'inr', 'rub', \n",
    "                'pln', 'huf', 'ron', 'try', 'brl', 'mxn', 'clp', 'cop', 'zar', 'ils', 'eur', 'gbp',\n",
    "                'jpy', 'aud', 'nzd', 'cad', 'chf', 'nok', 'sek', 'au$', 'ag$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward filing all NaN values\n",
    "df_v.fillna(method='ffill', inplace=True)\n",
    "df_v.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_list = pd.DataFrame(df_v.mean(), columns=['vol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidating all three features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=[usd_betas, equity_correlations]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vol_list.merge(X, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns = [\"vol\", \"usd_beta\", \"equity_corr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[[\"usd_beta\", \"equity_corr\", 'vol']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "mask = np.zeros_like(X.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(X.corr(), mask=mask, annot=True, cmap='coolwarm', ax=ax)\n",
    "ax.set_yticks(np.arange(0, df.shape[1])+0.1)\n",
    "ax.set_ylim([X.shape[1], 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardising the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed the arrays of scaled values into a DataFrame\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.pairplot(X)\n",
    "g.fig.set_size_inches(12,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.set_title('3D Scatterplot of FX Objects, Jan 2017 through Dec 2019', fontsize=14)\n",
    "\n",
    "origin = [0,0,0]\n",
    "ax.text(origin[0],origin[0],origin[0],\"origin\",size=15, color='navy')\n",
    "\n",
    "x_points = X_scaled.usd_beta\n",
    "y_points = X_scaled.equity_corr\n",
    "z_points = X_scaled.vol\n",
    "ax.scatter3D(x_points, y_points, z_points, s=150, color='maroon')\n",
    "\n",
    "ax.set_xlim(-2, 2.5)\n",
    "ax.set_ylim(-1.5, 3.5)\n",
    "ax.set_zlim(-1.5, 3)\n",
    "\n",
    "ax.set_xlabel('USD beta',labelpad=10,fontsize='large')\n",
    "ax.set_ylabel('Equity correlation',labelpad=10,fontsize='large')\n",
    "ax.set_zlabel('Implied volatility',labelpad=10,fontsize='large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to the \"medium_2019_interactive\" notebook for the interactive 3D charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_comp = linkage(X_scaled, method='complete', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_average = linkage(X_scaled, method='average', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_ward = linkage(X_scaled, method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the chart style...\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Dendrogram of FX Clusters, Jan 2017 through Dec 2019 (Complete)', fontsize=14)\n",
    "plt.xlabel('Distance', fontsize=10)\n",
    "plt.ylabel('Currency', fontsize=10)\n",
    "dendrogram(\n",
    "    hier_comp,\n",
    "    orientation='right',\n",
    "    #     leaf_rotation=90.,\n",
    "    leaf_font_size=20,\n",
    "    labels=X.index.values,\n",
    "    color_threshold=3\n",
    ")\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Dendrogram of FX Clusters, Jan 2017 through Dec 2019 (Average)', fontsize=14)\n",
    "plt.xlabel('Distance', fontsize=10)\n",
    "plt.ylabel('Currency', fontsize=10)\n",
    "dendrogram(\n",
    "    hier_average,\n",
    "    orientation='right',\n",
    "    #     leaf_rotation=90.,\n",
    "    leaf_font_size=20,\n",
    "    labels=X.index.values,\n",
    "    color_threshold=2.1\n",
    ")\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Dendrogram of FX Clusters, Jan 2017 through Dec 2019 (Ward)', fontsize=14)\n",
    "plt.xlabel('Distance', fontsize=10)\n",
    "plt.ylabel('Currency', fontsize=10)\n",
    "dendrogram(\n",
    "    hier_ward,\n",
    "    orientation='right',\n",
    "    #     leaf_rotation=90.,\n",
    "    leaf_font_size=20,\n",
    "    labels=X.index.values,\n",
    "    color_threshold=5\n",
    ")\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The various linkages - Complete, Average & Ward - all result in the same final 4 clusters. So no need to go into inter-cluster comparisons with the cophenetic correlations. In any event, the Complete linkage has the best scores among the three. So that's just use that as the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_comp = linkage(X_scaled, method='complete', metric='euclidean')\n",
    "c, coph_dists = cophenet(hier_comp, pdist(X, metric='euclidean'))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_average = linkage(X_scaled, method='average', metric='euclidean')\n",
    "c, coph_dists = cophenet(hier_average, pdist(X, metric='euclidean'))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_ward = linkage(X_scaled, method='ward', metric='euclidean')\n",
    "c, coph_dists = cophenet(hier_ward, pdist(X, metric='euclidean'))\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dendrogram (Complete linkage) above shows us how the bottoms-up agglomerative algorithm groups various currencies together during calendar years 2017 through 2019.\n",
    "\n",
    "#### Four clusters:\n",
    "#### Cluster 0 - NOK, CLP, AUD, SEK, NZD, HUF, PLN, EUR, RON, GBP, COP, MXN, BRL, RUB\n",
    "#### Cluster 1 - silver, TRY, ZAR\n",
    "#### Cluster 2 - gold, CHF, JPY\n",
    "#### Cluster 3 - INR, THB, PHP, MYR, TWD, ILS, CNY, SGD, IDR, KRW, CAD\n",
    "\n",
    "#### The closest geographically distinct cluster is Cluster 3, which is composed of mainly Asian currencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like k=4 gives the best silhouette score\n",
    "for k in range(2, 6):\n",
    "    model = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='complete')  \n",
    "    q = model.fit_predict(X_scaled)\n",
    "    s = silhouette_score(X_scaled, q, metric='euclidean')\n",
    "    print('{} number of clusters has Silhouette score of {:0.6f}'.format(k, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same when we use Average linkage\n",
    "for k in range(2, 6):\n",
    "    model = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='average')  \n",
    "    q = model.fit_predict(X_scaled)\n",
    "    s = silhouette_score(X_scaled, q, metric='euclidean')\n",
    "    print('{} number of clusters has Silhouette score of {:0.6f}'.format(k, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly different scores for the Ward linkage, due to the slight difference in results\n",
    "for k in range(2, 6):\n",
    "    model = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')  \n",
    "    q = model.fit_predict(X_scaled)\n",
    "    s = silhouette_score(X_scaled, q, metric='euclidean')\n",
    "    print('{} number of clusters has Silhouette score of {:0.6f}'.format(k, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we settle on the AH algorithm with Complete linkage and 4 clusters \n",
    "cluster_comp = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='complete')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['clusters'] = cluster_comp.fit_predict(X_scaled)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.pairplot(vars=[col for col in X if col!='clusters'], data=X, hue='clusters')\n",
    "g.fig.set_size_inches(12,9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The charts above makes clear the distinguishing features of the mainly Asian FX cluster (Cluster 3 - green), which includes ILS and CAD. It has low USD beta and low implied volatility, but a negative equity correlation.\n",
    "\n",
    "#### This may be contrasted with the \"safe haven cluster\" (Cluster 2 - yellow), which has a largely similarly low volatility and low USD beta, but is positively correlated to equities (the only cluster to be thus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.set_title('3D Scatterplot of FX Clusters, Jan 2017 through Dec 2019', fontsize=14)\n",
    "\n",
    "origin = [0,0,0]\n",
    "ax.text(origin[0],origin[0],origin[0],\"origin\",size=15, color='red')\n",
    "\n",
    "x_points = X_scaled.usd_beta\n",
    "y_points = X_scaled.equity_corr\n",
    "z_points = X_scaled.vol\n",
    "ax.scatter3D(x_points, y_points, z_points, s=150, c=X.clusters, cmap=\"viridis\")\n",
    "\n",
    "ax.set_xlim(-2, 2.5)\n",
    "ax.set_ylim(-1.5, 3.5)\n",
    "ax.set_zlim(-1.5, 3)\n",
    "\n",
    "ax.set_xlabel('USD beta',labelpad=10,fontsize='large')\n",
    "ax.set_ylabel('Equity correlation',labelpad=10,fontsize='large')\n",
    "ax.set_zlabel('Implied volatility',labelpad=10,fontsize='large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional characteristics of each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate the clusters through boxplots and two sample t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby('clusters').agg(['mean', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-plots for 'usd_beta'\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.boxplot(x='usd_beta', y='clusters', data=X, orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-plots for 'equity_corr'\n",
    "# Cluster 3 is statistically distinct from the others\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.boxplot(x='equity_corr', y='clusters', data=X, orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Box-plots for 'vol'\n",
    "# Cluster 0, cluster 1 and clusters 2-3 are statistically different from each other\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.boxplot(x='vol', y='clusters', data=X, orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run t-tests across two feature columns of DataFrame for two chosen clusters\n",
    "def cluster_test(i, j):\n",
    "    column_list = [x for x in X.columns if x != 'clusters']\n",
    "    t_test_results = {}\n",
    "    \n",
    "    for column in column_list:\n",
    "        group1 = X.where(X.clusters == i).dropna()[column]\n",
    "        group2 = X.where(X.clusters == j).dropna()[column]\n",
    "\n",
    "        t_test_results[column] = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(t_test_results, orient='Index')\n",
    "    results_df.columns = ['t-statistic','p-value']\n",
    "    print('Cluster {} versus Cluster {} t-test results:'.format(i, j))\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 0 is statistically distinct from Cluster 1 in usd_beta and vol\n",
    "cluster_test(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 0 is statistically distinct from Cluster 2 in usd_beta and equity_corr\n",
    "cluster_test(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 0 is statistically distinct from Cluster 3 in usd_beta and vol\n",
    "cluster_test(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 1 is statistically distinct from Cluster 2 in all three features\n",
    "cluster_test(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 1 is statistically distinct from Cluster 3 in usd_beta and vol\n",
    "cluster_test(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 2 is statistically distinct from Cluster 3 in all three features\n",
    "cluster_test(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The findings indicate that Cluster 1 (TRY, ZAR & silver) is distinguished by extremely high usd_beta and vol values. \n",
    "#### Cluster 2 (JPY, CHF & gold) is distinguished by positive equity_corr values. \n",
    "#### Cluster 3 (mostly Asian FX) tends to have the lowest usd_beta and vol values, though with some marginal overlap on these attributes with Cluster 2. \n",
    "#### Cluster 4 is the \"inbetweener\" cluster, particular on usd_beta and vol, with usd_beta values generally higher than 1.0, unlike Clusters 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results show that geography is of little use in segmenting global currencies in terms of their behavior to common financial market factors, except perhaps for non-JPY Asian currencies. The USD beta offered the best differencing factor across the four clusters, followed by implied volatility, and lastly by equity market correlation. However, one cluster (\"safe haven cluster\") was distinguished by having the only positive equity correlation values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
